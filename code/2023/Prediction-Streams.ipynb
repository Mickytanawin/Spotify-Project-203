{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy.signal as signal\n",
    "import skops.io as sio\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_SEED: int = 12345\n",
    "np.random.seed(RND_SEED)\n",
    "pd.core.common.random_state(RND_SEED)\n",
    "torch.set_default_device(\"cuda\")\n",
    "torch.manual_seed(RND_SEED)\n",
    "\n",
    "# Resolution for graph images\n",
    "WIDTH: int = 1366\n",
    "HEIGHT: int = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./../../data/Prepared-2023.csv\", encoding=\"utf-8\")\n",
    "df.loc[:, \"danceability_%\":\"speechiness_%\"] += 1\n",
    "df = df[(df[\"released_year\"] >= 2000) & (df[\"liveness_%\"] <= 65) & (df[\"artist_count\"] <= 4) & \\\n",
    "    (df[\"instrumentalness_%\"] <= 40) & (df[\"speechiness_%\"] <= 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list: list[str] = [\n",
    "    \"artist_count\", \"released_year\", \"released_month\", \"released_day\", \"bpm\", \"danceability_%\", \n",
    "    \"valence_%\", \"energy_%\", \"acousticness_%\", \"instrumentalness_%\", \"liveness_%\", \"speechiness_%\"\n",
    "]\n",
    "# for scale_name in scale_list:\n",
    "#     scaler = QuantileTransformer(output_distribution=\"normal\", random_state=RND_SEED, n_quantiles=700)\n",
    "#     scaler.fit(df[scale_name].to_numpy(dtype=np.float32)[:, np.newaxis])\n",
    "#     sio.dump(scaler, f\"./../../scalers/2023/{scale_name:s}.skops\")\n",
    "# del scale_name\n",
    "scalers: dict[str, QuantileTransformer] = {}\n",
    "for scale_name in scale_list:\n",
    "    scaler: QuantileTransformer = sio.load(f\"./../../scalers/2023/{scale_name:s}.skops\")\n",
    "    scalers[scale_name] = scaler\n",
    "    del scaler\n",
    "del scale_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scale_name in scale_list:\n",
    "    df[scale_name] = scalers[scale_name].transform(df[scale_name].to_numpy(dtype=np.float32)[:, np.newaxis])[:, 0]\n",
    "del scale_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP: int = 600\n",
    "\n",
    "features: list[str] = list(filter(lambda x: x != \"streams\", df.columns))\n",
    "X_1 = df.loc[df[\"streams\"] >= POP, features].to_numpy(dtype=np.float32)\n",
    "X_0 = df.loc[df[\"streams\"] < POP, features].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_tensor(x: np.ndarray) -> torch.Tensor:\n",
    "    return torch.tensor(x, requires_grad=False).to(\"cuda\")\n",
    "\n",
    "def split_data_np(x: np.ndarray, test: int) -> tuple[np.ndarray, ...]:\n",
    "    n_sample: int = x.shape[0]\n",
    "    n_test: int = math.floor(n_sample * (test / 100))\n",
    "    n_train: int = n_sample - n_test\n",
    "    return x[:n_train], x[n_train:]\n",
    "\n",
    "def split_x_y(x_0: np.ndarray, x_1: np.ndarray, test: int) -> tuple[torch.Tensor, ...]:\n",
    "    x_0_tr, x_0_te = split_data_np(x_0, test)\n",
    "    x_1_tr, x_1_te = split_data_np(x_1, test)\n",
    "    x_train = np.concat([x_0_tr, x_1_tr], axis=0)\n",
    "    y_train = np.concat([\n",
    "        np.zeros((x_0_tr.shape[0],), dtype=np.float32),\n",
    "        np.ones((x_1_tr.shape[0],), dtype=np.float32)\n",
    "    ],axis=0)[:, np.newaxis]\n",
    "    x_test = np.concat([x_0_te, x_1_te], axis=0)\n",
    "    y_test = np.concat([\n",
    "        np.zeros((x_0_te.shape[0],), dtype=np.float32),\n",
    "        np.ones((x_1_te.shape[0],), dtype=np.float32)\n",
    "    ],axis=0)[:, np.newaxis]\n",
    "    rnd_train = np.random.permutation(x_train.shape[0])\n",
    "    x_train, y_train = x_train[rnd_train], y_train[rnd_train]\n",
    "    rnd_test = np.random.permutation(x_test.shape[0])\n",
    "    x_test, y_test = x_test[rnd_test], y_test[rnd_test]\n",
    "    return np_to_tensor(x_train), np_to_tensor(x_test), np_to_tensor(y_train), np_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_x_y(X_0, X_1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SZ: int = 12\n",
    "EPOCHS: int = 300\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(x_train, y_train),\n",
    "    batch_size=BATCH_SZ,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator(device=\"cuda\")\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(x_test, y_test),\n",
    "    batch_size=BATCH_SZ * 2,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator(device=\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class Basic_ANN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.mish_ann = nn.Sequential(\n",
    "            nn.Linear(273, 128, device=\"cuda\"),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64, device=\"cuda\"),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16, device=\"cuda\"),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, 1, device=\"cuda\"),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.mish_ann.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mish_ann(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(loader: DataLoader, model, loss_fn, optimizer) -> None:\n",
    "    full_data: int = len(loader.dataset)\n",
    "    model.train()\n",
    "    loss_sum: float = 0.0\n",
    "    for batch, (x, y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        cross_loss = loss_fn(pred, y)\n",
    "        l2_regularization = 0.015 * torch.norm(torch.cat([x.view(-1) for x in model.parameters()]), 2)\n",
    "        loss = cross_loss + l2_regularization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_sum += loss.item()\n",
    "        if (batch + 1) % 10 == 0:\n",
    "            current = batch * BATCH_SZ + len(x)\n",
    "            print(f\"loss: {loss_sum / 10:>7f}  [{current:>5d}/{full_data:>5d}]\")\n",
    "            loss_sum = 0.0\n",
    "\n",
    "def test_loop(loader: DataLoader, model, loss_fn) -> None:\n",
    "    model.eval()\n",
    "    num_batches = len(loader)\n",
    "    test_loss: float = 0.0\n",
    "    test_correct: int = 0\n",
    "    n_samples: int = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            pred = model(x)\n",
    "            l2_regularization = 0.015 * torch.norm(torch.cat([x.view(-1) for x in model.parameters()]), 2)\n",
    "            test_loss += (loss_fn(pred, y) + l2_regularization).item()\n",
    "            pred_cls = (pred >= 0.5)\n",
    "            test_correct += (pred_cls == y).sum()\n",
    "            n_samples += y.shape[0]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} Accuracy: {test_correct / n_samples:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.962414  [  120/  571]\n",
      "loss: 0.886769  [  240/  571]\n",
      "loss: 0.891614  [  360/  571]\n",
      "loss: 0.884862  [  480/  571]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.879697  [  120/  571]\n",
      "loss: 0.919138  [  240/  571]\n",
      "loss: 0.829851  [  360/  571]\n",
      "loss: 0.841512  [  480/  571]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.851992  [  120/  571]\n",
      "loss: 0.861932  [  240/  571]\n",
      "loss: 0.867177  [  360/  571]\n",
      "loss: 0.834961  [  480/  571]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.841480  [  120/  571]\n",
      "loss: 0.828753  [  240/  571]\n",
      "loss: 0.854864  [  360/  571]\n",
      "loss: 0.828871  [  480/  571]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.783658  [  120/  571]\n",
      "loss: 0.877021  [  240/  571]\n",
      "loss: 0.744530  [  360/  571]\n",
      "loss: 0.836671  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.807122 Accuracy: 0.746479\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.789720  [  120/  571]\n",
      "loss: 0.835195  [  240/  571]\n",
      "loss: 0.824831  [  360/  571]\n",
      "loss: 0.787084  [  480/  571]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.783040  [  120/  571]\n",
      "loss: 0.766083  [  240/  571]\n",
      "loss: 0.817569  [  360/  571]\n",
      "loss: 0.764879  [  480/  571]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.769280  [  120/  571]\n",
      "loss: 0.819799  [  240/  571]\n",
      "loss: 0.765179  [  360/  571]\n",
      "loss: 0.783208  [  480/  571]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.809148  [  120/  571]\n",
      "loss: 0.750505  [  240/  571]\n",
      "loss: 0.768131  [  360/  571]\n",
      "loss: 0.757265  [  480/  571]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.799532  [  120/  571]\n",
      "loss: 0.751036  [  240/  571]\n",
      "loss: 0.778327  [  360/  571]\n",
      "loss: 0.729476  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.762781 Accuracy: 0.767606\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.757571  [  120/  571]\n",
      "loss: 0.747820  [  240/  571]\n",
      "loss: 0.734909  [  360/  571]\n",
      "loss: 0.732417  [  480/  571]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.796090  [  120/  571]\n",
      "loss: 0.711278  [  240/  571]\n",
      "loss: 0.740400  [  360/  571]\n",
      "loss: 0.697495  [  480/  571]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.717238  [  120/  571]\n",
      "loss: 0.737317  [  240/  571]\n",
      "loss: 0.751755  [  360/  571]\n",
      "loss: 0.761574  [  480/  571]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.700100  [  120/  571]\n",
      "loss: 0.659068  [  240/  571]\n",
      "loss: 0.722826  [  360/  571]\n",
      "loss: 0.759921  [  480/  571]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.744604  [  120/  571]\n",
      "loss: 0.693653  [  240/  571]\n",
      "loss: 0.668084  [  360/  571]\n",
      "loss: 0.708847  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.719566 Accuracy: 0.809859\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.694589  [  120/  571]\n",
      "loss: 0.638356  [  240/  571]\n",
      "loss: 0.724540  [  360/  571]\n",
      "loss: 0.746921  [  480/  571]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.666457  [  120/  571]\n",
      "loss: 0.721599  [  240/  571]\n",
      "loss: 0.646855  [  360/  571]\n",
      "loss: 0.686955  [  480/  571]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.690884  [  120/  571]\n",
      "loss: 0.675430  [  240/  571]\n",
      "loss: 0.646944  [  360/  571]\n",
      "loss: 0.654198  [  480/  571]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.676094  [  120/  571]\n",
      "loss: 0.653904  [  240/  571]\n",
      "loss: 0.661066  [  360/  571]\n",
      "loss: 0.715650  [  480/  571]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.646455  [  120/  571]\n",
      "loss: 0.668910  [  240/  571]\n",
      "loss: 0.652641  [  360/  571]\n",
      "loss: 0.642318  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.683448 Accuracy: 0.830986\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.594260  [  120/  571]\n",
      "loss: 0.651018  [  240/  571]\n",
      "loss: 0.669156  [  360/  571]\n",
      "loss: 0.679596  [  480/  571]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.597995  [  120/  571]\n",
      "loss: 0.762122  [  240/  571]\n",
      "loss: 0.580751  [  360/  571]\n",
      "loss: 0.638282  [  480/  571]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.659566  [  120/  571]\n",
      "loss: 0.662044  [  240/  571]\n",
      "loss: 0.635122  [  360/  571]\n",
      "loss: 0.657833  [  480/  571]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.649176  [  120/  571]\n",
      "loss: 0.629192  [  240/  571]\n",
      "loss: 0.660487  [  360/  571]\n",
      "loss: 0.610480  [  480/  571]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.631694  [  120/  571]\n",
      "loss: 0.669460  [  240/  571]\n",
      "loss: 0.586241  [  360/  571]\n",
      "loss: 0.655997  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.656200 Accuracy: 0.838028\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.621176  [  120/  571]\n",
      "loss: 0.616954  [  240/  571]\n",
      "loss: 0.646993  [  360/  571]\n",
      "loss: 0.622474  [  480/  571]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.625630  [  120/  571]\n",
      "loss: 0.693625  [  240/  571]\n",
      "loss: 0.602252  [  360/  571]\n",
      "loss: 0.579364  [  480/  571]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.660437  [  120/  571]\n",
      "loss: 0.573714  [  240/  571]\n",
      "loss: 0.596747  [  360/  571]\n",
      "loss: 0.617601  [  480/  571]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.591915  [  120/  571]\n",
      "loss: 0.585537  [  240/  571]\n",
      "loss: 0.621639  [  360/  571]\n",
      "loss: 0.594040  [  480/  571]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.757040  [  120/  571]\n",
      "loss: 0.570712  [  240/  571]\n",
      "loss: 0.542322  [  360/  571]\n",
      "loss: 0.547669  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.632305 Accuracy: 0.866197\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.602928  [  120/  571]\n",
      "loss: 0.651859  [  240/  571]\n",
      "loss: 0.589737  [  360/  571]\n",
      "loss: 0.579136  [  480/  571]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.591338  [  120/  571]\n",
      "loss: 0.557235  [  240/  571]\n",
      "loss: 0.543682  [  360/  571]\n",
      "loss: 0.608197  [  480/  571]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.504959  [  120/  571]\n",
      "loss: 0.523700  [  240/  571]\n",
      "loss: 0.621383  [  360/  571]\n",
      "loss: 0.590224  [  480/  571]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.637675  [  120/  571]\n",
      "loss: 0.556561  [  240/  571]\n",
      "loss: 0.560905  [  360/  571]\n",
      "loss: 0.590234  [  480/  571]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.606624  [  120/  571]\n",
      "loss: 0.531267  [  240/  571]\n",
      "loss: 0.591982  [  360/  571]\n",
      "loss: 0.611515  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.614802 Accuracy: 0.859155\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.550744  [  120/  571]\n",
      "loss: 0.525378  [  240/  571]\n",
      "loss: 0.576613  [  360/  571]\n",
      "loss: 0.568790  [  480/  571]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.621464  [  120/  571]\n",
      "loss: 0.577429  [  240/  571]\n",
      "loss: 0.566250  [  360/  571]\n",
      "loss: 0.534350  [  480/  571]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.583025  [  120/  571]\n",
      "loss: 0.553755  [  240/  571]\n",
      "loss: 0.574702  [  360/  571]\n",
      "loss: 0.575241  [  480/  571]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.596394  [  120/  571]\n",
      "loss: 0.640440  [  240/  571]\n",
      "loss: 0.567998  [  360/  571]\n",
      "loss: 0.529945  [  480/  571]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.592185  [  120/  571]\n",
      "loss: 0.595327  [  240/  571]\n",
      "loss: 0.513143  [  360/  571]\n",
      "loss: 0.589190  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.607953 Accuracy: 0.859155\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.594556  [  120/  571]\n",
      "loss: 0.608906  [  240/  571]\n",
      "loss: 0.534047  [  360/  571]\n",
      "loss: 0.548181  [  480/  571]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.573965  [  120/  571]\n",
      "loss: 0.600867  [  240/  571]\n",
      "loss: 0.548290  [  360/  571]\n",
      "loss: 0.530344  [  480/  571]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.575897  [  120/  571]\n",
      "loss: 0.562505  [  240/  571]\n",
      "loss: 0.592109  [  360/  571]\n",
      "loss: 0.495528  [  480/  571]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.540900  [  120/  571]\n",
      "loss: 0.530082  [  240/  571]\n",
      "loss: 0.524353  [  360/  571]\n",
      "loss: 0.525743  [  480/  571]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.542734  [  120/  571]\n",
      "loss: 0.578677  [  240/  571]\n",
      "loss: 0.527020  [  360/  571]\n",
      "loss: 0.556547  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.597928 Accuracy: 0.866197\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.552626  [  120/  571]\n",
      "loss: 0.527348  [  240/  571]\n",
      "loss: 0.518953  [  360/  571]\n",
      "loss: 0.553606  [  480/  571]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.563311  [  120/  571]\n",
      "loss: 0.581805  [  240/  571]\n",
      "loss: 0.530070  [  360/  571]\n",
      "loss: 0.556828  [  480/  571]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.468130  [  120/  571]\n",
      "loss: 0.492073  [  240/  571]\n",
      "loss: 0.529057  [  360/  571]\n",
      "loss: 0.609097  [  480/  571]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.590846  [  120/  571]\n",
      "loss: 0.499042  [  240/  571]\n",
      "loss: 0.514780  [  360/  571]\n",
      "loss: 0.543012  [  480/  571]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.505829  [  120/  571]\n",
      "loss: 0.605447  [  240/  571]\n",
      "loss: 0.580166  [  360/  571]\n",
      "loss: 0.497469  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.594146 Accuracy: 0.859155\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.500149  [  120/  571]\n",
      "loss: 0.534596  [  240/  571]\n",
      "loss: 0.523206  [  360/  571]\n",
      "loss: 0.522011  [  480/  571]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.509355  [  120/  571]\n",
      "loss: 0.494035  [  240/  571]\n",
      "loss: 0.560452  [  360/  571]\n",
      "loss: 0.551631  [  480/  571]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.522762  [  120/  571]\n",
      "loss: 0.562349  [  240/  571]\n",
      "loss: 0.522909  [  360/  571]\n",
      "loss: 0.487640  [  480/  571]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.482472  [  120/  571]\n",
      "loss: 0.490706  [  240/  571]\n",
      "loss: 0.554214  [  360/  571]\n",
      "loss: 0.598544  [  480/  571]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.599946  [  120/  571]\n",
      "loss: 0.505345  [  240/  571]\n",
      "loss: 0.480687  [  360/  571]\n",
      "loss: 0.530820  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.590135 Accuracy: 0.859155\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.565841  [  120/  571]\n",
      "loss: 0.587907  [  240/  571]\n",
      "loss: 0.506486  [  360/  571]\n",
      "loss: 0.525824  [  480/  571]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.500068  [  120/  571]\n",
      "loss: 0.570752  [  240/  571]\n",
      "loss: 0.588902  [  360/  571]\n",
      "loss: 0.499944  [  480/  571]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.494349  [  120/  571]\n",
      "loss: 0.527731  [  240/  571]\n",
      "loss: 0.555794  [  360/  571]\n",
      "loss: 0.496942  [  480/  571]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.537202  [  120/  571]\n",
      "loss: 0.547519  [  240/  571]\n",
      "loss: 0.501921  [  360/  571]\n",
      "loss: 0.497197  [  480/  571]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.595445  [  120/  571]\n",
      "loss: 0.481231  [  240/  571]\n",
      "loss: 0.479507  [  360/  571]\n",
      "loss: 0.537909  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.591371 Accuracy: 0.859155\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.607903  [  120/  571]\n",
      "loss: 0.491581  [  240/  571]\n",
      "loss: 0.458925  [  360/  571]\n",
      "loss: 0.519321  [  480/  571]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.519973  [  120/  571]\n",
      "loss: 0.544718  [  240/  571]\n",
      "loss: 0.439172  [  360/  571]\n",
      "loss: 0.573372  [  480/  571]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.509044  [  120/  571]\n",
      "loss: 0.529063  [  240/  571]\n",
      "loss: 0.511412  [  360/  571]\n",
      "loss: 0.551438  [  480/  571]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.452664  [  120/  571]\n",
      "loss: 0.468741  [  240/  571]\n",
      "loss: 0.563593  [  360/  571]\n",
      "loss: 0.519528  [  480/  571]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.509643  [  120/  571]\n",
      "loss: 0.534583  [  240/  571]\n",
      "loss: 0.459373  [  360/  571]\n",
      "loss: 0.457137  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.593470 Accuracy: 0.859155\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.439318  [  120/  571]\n",
      "loss: 0.508576  [  240/  571]\n",
      "loss: 0.548762  [  360/  571]\n",
      "loss: 0.557614  [  480/  571]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.481929  [  120/  571]\n",
      "loss: 0.477214  [  240/  571]\n",
      "loss: 0.484894  [  360/  571]\n",
      "loss: 0.545171  [  480/  571]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.465208  [  120/  571]\n",
      "loss: 0.542341  [  240/  571]\n",
      "loss: 0.498267  [  360/  571]\n",
      "loss: 0.496110  [  480/  571]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.493841  [  120/  571]\n",
      "loss: 0.482682  [  240/  571]\n",
      "loss: 0.465396  [  360/  571]\n",
      "loss: 0.499956  [  480/  571]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.488579  [  120/  571]\n",
      "loss: 0.486634  [  240/  571]\n",
      "loss: 0.485866  [  360/  571]\n",
      "loss: 0.502560  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.589828 Accuracy: 0.859155\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.515468  [  120/  571]\n",
      "loss: 0.484851  [  240/  571]\n",
      "loss: 0.440374  [  360/  571]\n",
      "loss: 0.491589  [  480/  571]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.513258  [  120/  571]\n",
      "loss: 0.478103  [  240/  571]\n",
      "loss: 0.492185  [  360/  571]\n",
      "loss: 0.454860  [  480/  571]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.450989  [  120/  571]\n",
      "loss: 0.497251  [  240/  571]\n",
      "loss: 0.472901  [  360/  571]\n",
      "loss: 0.572024  [  480/  571]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.503863  [  120/  571]\n",
      "loss: 0.401113  [  240/  571]\n",
      "loss: 0.513907  [  360/  571]\n",
      "loss: 0.457994  [  480/  571]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.475385  [  120/  571]\n",
      "loss: 0.505337  [  240/  571]\n",
      "loss: 0.500097  [  360/  571]\n",
      "loss: 0.537635  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.592851 Accuracy: 0.859155\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.508132  [  120/  571]\n",
      "loss: 0.513652  [  240/  571]\n",
      "loss: 0.506193  [  360/  571]\n",
      "loss: 0.420920  [  480/  571]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.512671  [  120/  571]\n",
      "loss: 0.451691  [  240/  571]\n",
      "loss: 0.498202  [  360/  571]\n",
      "loss: 0.449042  [  480/  571]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.455238  [  120/  571]\n",
      "loss: 0.480809  [  240/  571]\n",
      "loss: 0.473131  [  360/  571]\n",
      "loss: 0.505324  [  480/  571]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.430027  [  120/  571]\n",
      "loss: 0.491049  [  240/  571]\n",
      "loss: 0.435472  [  360/  571]\n",
      "loss: 0.532065  [  480/  571]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.561377  [  120/  571]\n",
      "loss: 0.456481  [  240/  571]\n",
      "loss: 0.535333  [  360/  571]\n",
      "loss: 0.427746  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.594766 Accuracy: 0.859155\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.500827  [  120/  571]\n",
      "loss: 0.423383  [  240/  571]\n",
      "loss: 0.514401  [  360/  571]\n",
      "loss: 0.481558  [  480/  571]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.487150  [  120/  571]\n",
      "loss: 0.441873  [  240/  571]\n",
      "loss: 0.527913  [  360/  571]\n",
      "loss: 0.407335  [  480/  571]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.531183  [  120/  571]\n",
      "loss: 0.462905  [  240/  571]\n",
      "loss: 0.510141  [  360/  571]\n",
      "loss: 0.406559  [  480/  571]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.451576  [  120/  571]\n",
      "loss: 0.448512  [  240/  571]\n",
      "loss: 0.506326  [  360/  571]\n",
      "loss: 0.460603  [  480/  571]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.463975  [  120/  571]\n",
      "loss: 0.444674  [  240/  571]\n",
      "loss: 0.498745  [  360/  571]\n",
      "loss: 0.476620  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.595118 Accuracy: 0.852113\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.456816  [  120/  571]\n",
      "loss: 0.490292  [  240/  571]\n",
      "loss: 0.435952  [  360/  571]\n",
      "loss: 0.464451  [  480/  571]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.416360  [  120/  571]\n",
      "loss: 0.510565  [  240/  571]\n",
      "loss: 0.479188  [  360/  571]\n",
      "loss: 0.516047  [  480/  571]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.425722  [  120/  571]\n",
      "loss: 0.523569  [  240/  571]\n",
      "loss: 0.441007  [  360/  571]\n",
      "loss: 0.536217  [  480/  571]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.446580  [  120/  571]\n",
      "loss: 0.547356  [  240/  571]\n",
      "loss: 0.439009  [  360/  571]\n",
      "loss: 0.484141  [  480/  571]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.451950  [  120/  571]\n",
      "loss: 0.475937  [  240/  571]\n",
      "loss: 0.509981  [  360/  571]\n",
      "loss: 0.474985  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.593068 Accuracy: 0.852113\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.457710  [  120/  571]\n",
      "loss: 0.518984  [  240/  571]\n",
      "loss: 0.468858  [  360/  571]\n",
      "loss: 0.437326  [  480/  571]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.463954  [  120/  571]\n",
      "loss: 0.476187  [  240/  571]\n",
      "loss: 0.389312  [  360/  571]\n",
      "loss: 0.536954  [  480/  571]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.441600  [  120/  571]\n",
      "loss: 0.450324  [  240/  571]\n",
      "loss: 0.453392  [  360/  571]\n",
      "loss: 0.477835  [  480/  571]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.477089  [  120/  571]\n",
      "loss: 0.461368  [  240/  571]\n",
      "loss: 0.480870  [  360/  571]\n",
      "loss: 0.538977  [  480/  571]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.477902  [  120/  571]\n",
      "loss: 0.485381  [  240/  571]\n",
      "loss: 0.504409  [  360/  571]\n",
      "loss: 0.471038  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.598248 Accuracy: 0.852113\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.453311  [  120/  571]\n",
      "loss: 0.512710  [  240/  571]\n",
      "loss: 0.466834  [  360/  571]\n",
      "loss: 0.442916  [  480/  571]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.476297  [  120/  571]\n",
      "loss: 0.478770  [  240/  571]\n",
      "loss: 0.448744  [  360/  571]\n",
      "loss: 0.483631  [  480/  571]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.475842  [  120/  571]\n",
      "loss: 0.484992  [  240/  571]\n",
      "loss: 0.478670  [  360/  571]\n",
      "loss: 0.468417  [  480/  571]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.464727  [  120/  571]\n",
      "loss: 0.483114  [  240/  571]\n",
      "loss: 0.377233  [  360/  571]\n",
      "loss: 0.494436  [  480/  571]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.488074  [  120/  571]\n",
      "loss: 0.419238  [  240/  571]\n",
      "loss: 0.477870  [  360/  571]\n",
      "loss: 0.430478  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.600881 Accuracy: 0.852113\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.486630  [  120/  571]\n",
      "loss: 0.498147  [  240/  571]\n",
      "loss: 0.427453  [  360/  571]\n",
      "loss: 0.414778  [  480/  571]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.451947  [  120/  571]\n",
      "loss: 0.392430  [  240/  571]\n",
      "loss: 0.487001  [  360/  571]\n",
      "loss: 0.509521  [  480/  571]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.536095  [  120/  571]\n",
      "loss: 0.454863  [  240/  571]\n",
      "loss: 0.463631  [  360/  571]\n",
      "loss: 0.515875  [  480/  571]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.418991  [  120/  571]\n",
      "loss: 0.455922  [  240/  571]\n",
      "loss: 0.487262  [  360/  571]\n",
      "loss: 0.534544  [  480/  571]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.460990  [  120/  571]\n",
      "loss: 0.410023  [  240/  571]\n",
      "loss: 0.533847  [  360/  571]\n",
      "loss: 0.402276  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.602864 Accuracy: 0.852113\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.436709  [  120/  571]\n",
      "loss: 0.492812  [  240/  571]\n",
      "loss: 0.454995  [  360/  571]\n",
      "loss: 0.499158  [  480/  571]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.459729  [  120/  571]\n",
      "loss: 0.475245  [  240/  571]\n",
      "loss: 0.462404  [  360/  571]\n",
      "loss: 0.423669  [  480/  571]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.509762  [  120/  571]\n",
      "loss: 0.416099  [  240/  571]\n",
      "loss: 0.540441  [  360/  571]\n",
      "loss: 0.451414  [  480/  571]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.448408  [  120/  571]\n",
      "loss: 0.419685  [  240/  571]\n",
      "loss: 0.477387  [  360/  571]\n",
      "loss: 0.519822  [  480/  571]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.451088  [  120/  571]\n",
      "loss: 0.435824  [  240/  571]\n",
      "loss: 0.430861  [  360/  571]\n",
      "loss: 0.542158  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.606378 Accuracy: 0.845070\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.471273  [  120/  571]\n",
      "loss: 0.446206  [  240/  571]\n",
      "loss: 0.432800  [  360/  571]\n",
      "loss: 0.436944  [  480/  571]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.452252  [  120/  571]\n",
      "loss: 0.478957  [  240/  571]\n",
      "loss: 0.431926  [  360/  571]\n",
      "loss: 0.435753  [  480/  571]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.522990  [  120/  571]\n",
      "loss: 0.479921  [  240/  571]\n",
      "loss: 0.391875  [  360/  571]\n",
      "loss: 0.426152  [  480/  571]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.424323  [  120/  571]\n",
      "loss: 0.469742  [  240/  571]\n",
      "loss: 0.469916  [  360/  571]\n",
      "loss: 0.435778  [  480/  571]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.449316  [  120/  571]\n",
      "loss: 0.484127  [  240/  571]\n",
      "loss: 0.475986  [  360/  571]\n",
      "loss: 0.430134  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.604859 Accuracy: 0.845070\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.412064  [  120/  571]\n",
      "loss: 0.475990  [  240/  571]\n",
      "loss: 0.476431  [  360/  571]\n",
      "loss: 0.426818  [  480/  571]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.468815  [  120/  571]\n",
      "loss: 0.418571  [  240/  571]\n",
      "loss: 0.496005  [  360/  571]\n",
      "loss: 0.507687  [  480/  571]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.459503  [  120/  571]\n",
      "loss: 0.414896  [  240/  571]\n",
      "loss: 0.433811  [  360/  571]\n",
      "loss: 0.472097  [  480/  571]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.397554  [  120/  571]\n",
      "loss: 0.413745  [  240/  571]\n",
      "loss: 0.577162  [  360/  571]\n",
      "loss: 0.453977  [  480/  571]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.461321  [  120/  571]\n",
      "loss: 0.427329  [  240/  571]\n",
      "loss: 0.401911  [  360/  571]\n",
      "loss: 0.472641  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.611759 Accuracy: 0.852113\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.455120  [  120/  571]\n",
      "loss: 0.452110  [  240/  571]\n",
      "loss: 0.483232  [  360/  571]\n",
      "loss: 0.462370  [  480/  571]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.494880  [  120/  571]\n",
      "loss: 0.516233  [  240/  571]\n",
      "loss: 0.448046  [  360/  571]\n",
      "loss: 0.386142  [  480/  571]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.488692  [  120/  571]\n",
      "loss: 0.457163  [  240/  571]\n",
      "loss: 0.459534  [  360/  571]\n",
      "loss: 0.492252  [  480/  571]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.473720  [  120/  571]\n",
      "loss: 0.489776  [  240/  571]\n",
      "loss: 0.431680  [  360/  571]\n",
      "loss: 0.407417  [  480/  571]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.407644  [  120/  571]\n",
      "loss: 0.440024  [  240/  571]\n",
      "loss: 0.445711  [  360/  571]\n",
      "loss: 0.463141  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.617567 Accuracy: 0.845070\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.406103  [  120/  571]\n",
      "loss: 0.506713  [  240/  571]\n",
      "loss: 0.469919  [  360/  571]\n",
      "loss: 0.414103  [  480/  571]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.427102  [  120/  571]\n",
      "loss: 0.444104  [  240/  571]\n",
      "loss: 0.437425  [  360/  571]\n",
      "loss: 0.441313  [  480/  571]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.448915  [  120/  571]\n",
      "loss: 0.489449  [  240/  571]\n",
      "loss: 0.448310  [  360/  571]\n",
      "loss: 0.424290  [  480/  571]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.504974  [  120/  571]\n",
      "loss: 0.406531  [  240/  571]\n",
      "loss: 0.465937  [  360/  571]\n",
      "loss: 0.422924  [  480/  571]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.412891  [  120/  571]\n",
      "loss: 0.418118  [  240/  571]\n",
      "loss: 0.468328  [  360/  571]\n",
      "loss: 0.512618  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.613469 Accuracy: 0.845070\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.447323  [  120/  571]\n",
      "loss: 0.391878  [  240/  571]\n",
      "loss: 0.464820  [  360/  571]\n",
      "loss: 0.510880  [  480/  571]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.473316  [  120/  571]\n",
      "loss: 0.453721  [  240/  571]\n",
      "loss: 0.504766  [  360/  571]\n",
      "loss: 0.422339  [  480/  571]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.453758  [  120/  571]\n",
      "loss: 0.452417  [  240/  571]\n",
      "loss: 0.421948  [  360/  571]\n",
      "loss: 0.465117  [  480/  571]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.426540  [  120/  571]\n",
      "loss: 0.445212  [  240/  571]\n",
      "loss: 0.420540  [  360/  571]\n",
      "loss: 0.448329  [  480/  571]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.488463  [  120/  571]\n",
      "loss: 0.484148  [  240/  571]\n",
      "loss: 0.454446  [  360/  571]\n",
      "loss: 0.440027  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.614620 Accuracy: 0.845070\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.405092  [  120/  571]\n",
      "loss: 0.417752  [  240/  571]\n",
      "loss: 0.424219  [  360/  571]\n",
      "loss: 0.444803  [  480/  571]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.468904  [  120/  571]\n",
      "loss: 0.356084  [  240/  571]\n",
      "loss: 0.428992  [  360/  571]\n",
      "loss: 0.462242  [  480/  571]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.436943  [  120/  571]\n",
      "loss: 0.454096  [  240/  571]\n",
      "loss: 0.442462  [  360/  571]\n",
      "loss: 0.441558  [  480/  571]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.476129  [  120/  571]\n",
      "loss: 0.472313  [  240/  571]\n",
      "loss: 0.448974  [  360/  571]\n",
      "loss: 0.399903  [  480/  571]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.436877  [  120/  571]\n",
      "loss: 0.403960  [  240/  571]\n",
      "loss: 0.443814  [  360/  571]\n",
      "loss: 0.506256  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.614555 Accuracy: 0.845070\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.413223  [  120/  571]\n",
      "loss: 0.464334  [  240/  571]\n",
      "loss: 0.409343  [  360/  571]\n",
      "loss: 0.498601  [  480/  571]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.466330  [  120/  571]\n",
      "loss: 0.460954  [  240/  571]\n",
      "loss: 0.462218  [  360/  571]\n",
      "loss: 0.469741  [  480/  571]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.479206  [  120/  571]\n",
      "loss: 0.374984  [  240/  571]\n",
      "loss: 0.468023  [  360/  571]\n",
      "loss: 0.490986  [  480/  571]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.450779  [  120/  571]\n",
      "loss: 0.450138  [  240/  571]\n",
      "loss: 0.454312  [  360/  571]\n",
      "loss: 0.426909  [  480/  571]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.482488  [  120/  571]\n",
      "loss: 0.424335  [  240/  571]\n",
      "loss: 0.447261  [  360/  571]\n",
      "loss: 0.411870  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.614433 Accuracy: 0.852113\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.445037  [  120/  571]\n",
      "loss: 0.441763  [  240/  571]\n",
      "loss: 0.428332  [  360/  571]\n",
      "loss: 0.416343  [  480/  571]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.438643  [  120/  571]\n",
      "loss: 0.500354  [  240/  571]\n",
      "loss: 0.432360  [  360/  571]\n",
      "loss: 0.447024  [  480/  571]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.371058  [  120/  571]\n",
      "loss: 0.398061  [  240/  571]\n",
      "loss: 0.417744  [  360/  571]\n",
      "loss: 0.485119  [  480/  571]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.432315  [  120/  571]\n",
      "loss: 0.507515  [  240/  571]\n",
      "loss: 0.421852  [  360/  571]\n",
      "loss: 0.426371  [  480/  571]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.445846  [  120/  571]\n",
      "loss: 0.454704  [  240/  571]\n",
      "loss: 0.405388  [  360/  571]\n",
      "loss: 0.453929  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.616026 Accuracy: 0.852113\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.438344  [  120/  571]\n",
      "loss: 0.435805  [  240/  571]\n",
      "loss: 0.411580  [  360/  571]\n",
      "loss: 0.397154  [  480/  571]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.460375  [  120/  571]\n",
      "loss: 0.481542  [  240/  571]\n",
      "loss: 0.454098  [  360/  571]\n",
      "loss: 0.431439  [  480/  571]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.432885  [  120/  571]\n",
      "loss: 0.477487  [  240/  571]\n",
      "loss: 0.406784  [  360/  571]\n",
      "loss: 0.396809  [  480/  571]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.469262  [  120/  571]\n",
      "loss: 0.428622  [  240/  571]\n",
      "loss: 0.462059  [  360/  571]\n",
      "loss: 0.406635  [  480/  571]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.492209  [  120/  571]\n",
      "loss: 0.364604  [  240/  571]\n",
      "loss: 0.458866  [  360/  571]\n",
      "loss: 0.480623  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.618813 Accuracy: 0.852113\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.413240  [  120/  571]\n",
      "loss: 0.471615  [  240/  571]\n",
      "loss: 0.471770  [  360/  571]\n",
      "loss: 0.466109  [  480/  571]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.450488  [  120/  571]\n",
      "loss: 0.440651  [  240/  571]\n",
      "loss: 0.468949  [  360/  571]\n",
      "loss: 0.450871  [  480/  571]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.470090  [  120/  571]\n",
      "loss: 0.471685  [  240/  571]\n",
      "loss: 0.439290  [  360/  571]\n",
      "loss: 0.422826  [  480/  571]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.455810  [  120/  571]\n",
      "loss: 0.415598  [  240/  571]\n",
      "loss: 0.536335  [  360/  571]\n",
      "loss: 0.426734  [  480/  571]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.410187  [  120/  571]\n",
      "loss: 0.469589  [  240/  571]\n",
      "loss: 0.456758  [  360/  571]\n",
      "loss: 0.459487  [  480/  571]\n",
      "Test Error: \n",
      " Avg loss: 0.621905 Accuracy: 0.852113\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.380846  [  120/  571]\n",
      "loss: 0.477209  [  240/  571]\n",
      "loss: 0.450243  [  360/  571]\n",
      "loss: 0.456131  [  480/  571]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.487950  [  120/  571]\n",
      "loss: 0.404101  [  240/  571]\n",
      "loss: 0.384819  [  360/  571]\n",
      "loss: 0.499959  [  480/  571]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.450529  [  120/  571]\n",
      "loss: 0.409907  [  240/  571]\n",
      "loss: 0.401640  [  360/  571]\n",
      "loss: 0.426127  [  480/  571]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(loader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      8\u001b[0m cross_loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m----> 9\u001b[0m l2_regularization \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.015\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_loss \u001b[38;5;241m+\u001b[39m l2_regularization\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:1596\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the matrix norm or vector norm of a given tensor.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[1;32m   1504\u001b[0m \u001b[38;5;124;03m.. warning::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m    (tensor(3.7417), tensor(11.2250))\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m-> 1596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# NB. All the repeated code and weird python is to please TorchScript.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;66;03m#     For a more compact implementation see the relevant function in `_refs/__init__.py`\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m \n\u001b[1;32m   1602\u001b[0m \u001b[38;5;66;03m# We don't do this for MPS or sparse tensors\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m \\\n\u001b[1;32m   1604\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mbackend_registration\u001b[38;5;241m.\u001b[39m_privateuse1_backend_name):\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/overrides.py:1630\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1630\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:1632\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1630\u001b[0m _p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m p\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\u001b[38;5;28minput\u001b[39m, _p, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Basic_ANN().to(\"cuda\")\n",
    "loss_fn = nn.BCELoss().to(\"cuda\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.025)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.985)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    scheduler.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loop(test_loader, model, loss_fn)\n",
    "del epoch\n",
    "print(\"Training Finished\\nTest Result:\")\n",
    "test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(209)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = (model(torch.tensor(np.concat([X_0, X_1], axis=0), device=\"cuda\")) >= 0.5).to(\"cpu\")\n",
    "    print(pred.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
